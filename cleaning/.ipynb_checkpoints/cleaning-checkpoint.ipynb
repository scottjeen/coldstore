{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "from codecarbon import track_emissions\n",
    "e_path = \"/Users/ScottJeen/OneDrive - University of Cambridge/Admin/phd_emissions\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coldstore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    \n",
    "    import glob\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    \n",
    "    data = {}\n",
    "    for fname in glob.glob(path):\n",
    "        xls = pd.ExcelFile(fname)\n",
    "        data_dict = pd.read_excel(xls, sheet_name=None)\n",
    "        name = re.search('([^\\/]+$)', fname).group(0).replace('.xls', '')\n",
    "\n",
    "        for i, (k, v) in enumerate(data_dict.items()):\n",
    "            new_key = v.columns[1]\n",
    "            count = str(i + 1)\n",
    "            v = v.rename(columns=v.iloc[0])\n",
    "            v = v.iloc[1:]\n",
    "\n",
    "            # format datatime\n",
    "            datetime_format = '%b %d, %Y %I:%M:%S %p'\n",
    "            v['Time Zone: America/New_York'] = pd.to_datetime(v['Time Zone: America/New_York'])\n",
    "\n",
    "            # drop status column\n",
    "            v = v.drop('Status',axis=1)\n",
    "\n",
    "            # rename features\n",
    "            v = v.rename(columns={v.columns[0]: 'Datetime', v.columns[1]: name + ' ' +count})\n",
    "            v = v.set_index('Datetime')\n",
    "\n",
    "            # remove multiple entries at each timestep\n",
    "            v = v[~v.index.duplicated(keep='first')]              \n",
    "\n",
    "            data[new_key] = v\n",
    "            \n",
    "    return data                                                         \n",
    "                                                                    \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "nov_path = \"/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/jonluca_data/nov_data/*.xls\"\n",
    "feb_path = \"/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/jonluca_data/feb_data/*.xls\"\n",
    "pickle_path = \"/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/jonluca_data/concat_data/data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "nov = get_data(nov_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb = get_data(feb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for k in nov:\n",
    "    old = nov[k]\n",
    "    new = feb[k]\n",
    "    new = new.rename(columns={new.columns[0]: old.columns[0]})\n",
    "    \n",
    "    concat = pd.concat([old, new], axis=0, join='outer', sort=True)\n",
    "\n",
    "    # remove multiple entries at each timestep\n",
    "    concat = concat[~concat.index.duplicated(keep='first')]\n",
    "    dfs.append(concat)\n",
    "\n",
    "# join columns on datatime and sort alphabetically\n",
    "data = dfs[0].join(dfs[1:], how='inner')\n",
    "data = data.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-specific cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join columns on datatime and sort alphabetically\n",
    "data = dfs[0].join(dfs[1:], how='inner')\n",
    "data = data.sort_index(axis=1)\n",
    "\n",
    "# convert to floats\n",
    "data = data.astype('float')\n",
    "\n",
    "# drop faulty freezer temperature sensor feature (these are the 'door inside' sensors, 6&7)\n",
    "# and the second outside freezer door sensors (9 & 10) that are redudant as we already have readings\n",
    "data = data.drop(['FREEZER SLAB TEMP 6',\\\n",
    "                  'FREEZER SLAB TEMP 7',\n",
    "                  'FREEZER SLAB TEMP 2',\n",
    "                  'FREEZER SLAB TEMP 10'], axis=1)\n",
    "\n",
    "# column specific renaming\n",
    "new_cols = {'FREEZER SLAB TEMP 1': 'FREEZER SLAB TEMP INSIDE RIGHT',\\\n",
    "        'FREEZER SLAB TEMP 3': 'FREEZER SOIL TEMP INSIDE RIGHT',\n",
    "        'FREEZER SLAB TEMP 4': 'FREEZER SLAB TEMP INSIDE LEFT',\n",
    "        'FREEZER SLAB TEMP 5': 'FREEZER SOIL TEMP INSIDE LEFT',\n",
    "        'FREEZER SLAB TEMP 8': 'FREEZER SLAB TEMP OUTSIDE DOOR',\n",
    "        'FREEZER SLAB TEMP 9': 'FREEZER SOIL TEMP OUTSIDE DOOR',\n",
    "        'COOLER SLAB TEMP 1': 'COOLER SLAB TEMP INSIDE 1',\\\n",
    "        'COOLER SLAB TEMP 2': 'COOLER SOIL TEMP INSIDE 1',\n",
    "        'COOLER SLAB TEMP 3': 'COOLER SLAB TEMP INSIDE 2',\n",
    "        'COOLER SLAB TEMP 4': 'COOLER SLAB TEMP OUTSIDE',\n",
    "       }\n",
    "data = data.rename(new_cols, axis=1)\n",
    "\n",
    "# normalize humidity features\n",
    "hum = data.columns.str.contains('HUMIDITY')\n",
    "data.loc[:,hum] = data.loc[:,hum] / 100\n",
    "\n",
    "# get power data from amps (power (kW) = amps * 600V / 1000)\n",
    "amp = data.columns.str.contains('COMP AMP')\n",
    "data.loc[:,amp] = data.loc[:,amp] * 600 / 1000\n",
    "\n",
    "# rename columns\n",
    "new_cols = pd.Series(data.columns).str.replace('AMP', 'POWER (kW)')\n",
    "new_cols = list(new_cols)\n",
    "data.columns = new_cols\n",
    "\n",
    "# add total power feature\n",
    "power_features = data.columns.str.contains('POWER')\n",
    "data['TOTAL POWER (kW)'] = data.loc[:,power_features].sum(axis=1)\n",
    "\n",
    "# add energy feature (assume power is constant for 3 minute period between datapoints)\n",
    "data['TOTAL ENERGY (kWh)'] = data['TOTAL POWER (kW)'] * (60/3)\n",
    "\n",
    "# drop tail na values\n",
    "tail_bool = data.iloc[round(data.shape[0]*0.9):].isnull().any(axis=1)\n",
    "tail_na = tail_bool[tail_bool==True].index\n",
    "data = data.drop(tail_na, axis=0)\n",
    "\n",
    "# interpolate missing values\n",
    "data.interpolate(method='time', inplace=True, axis=0)\n",
    "\n",
    "# write to pickle\n",
    "data.to_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electricity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(pickle_path)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime convertor\n",
    "def datetime_conv(df, hour_format='%H', date_format='%Y-%m-%d', hour='Hour', date='Date'):\n",
    "    \n",
    "    # format hour feature to padded 24h \n",
    "    df[hour] = df[hour] - 1\n",
    "    df[hour] = df[hour].astype(str)\n",
    "    df[hour] = df[hour].str.pad(width=2, side='left', fillchar='0')\n",
    "\n",
    "    # convert to datetime\n",
    "    df[date] = pd.to_datetime(df[date], format=date_format)\n",
    "    df[hour] = pd.to_datetime(df[hour], format=hour_format)\n",
    "\n",
    "    x = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        d = row.loc[date].date()\n",
    "        t = row.loc[hour].time()\n",
    "        x.append(dt.datetime.combine(d, t))\n",
    "\n",
    "    df['Datetime'] = pd.Series(x)\n",
    "    \n",
    "    # drop old date and time cols\n",
    "    df = df.drop([hour, date], axis=1)\n",
    "    \n",
    "    # set index to datetime\n",
    "    df = df.set_index('Datetime')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# import elec data\n",
    "path_elec = '/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/elec_data/*.csv'\n",
    "\n",
    "dfs_elec = []\n",
    "files = [fname for fname in glob.glob(path_elec)]\n",
    "\n",
    "# read hourly price data\n",
    "hourly_price = pd.read_csv(files[0], header=3)\n",
    "hourly_price = hourly_price.drop(hourly_price.columns[6:], axis=1)\n",
    "\n",
    "# run datetime convertor\n",
    "hourly_price = datetime_conv(hourly_price, date_format='%d/%m/%Y')\n",
    "\n",
    "# rename columns\n",
    "cols = hourly_price.columns\n",
    "new_cols = {cols[0]: 'PRICE ($/MWH)',\\\n",
    "            cols[1]: '1 HOUR PRICE PREDICT',\\\n",
    "            cols[2]: '2 HOUR PRICE PREDICT',\\\n",
    "            cols[3]: '3 HOUR PRICE PREDICT'\n",
    "           }\n",
    "\n",
    "hourly_price = hourly_price.rename(new_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read elec supply data\n",
    "hourly_supply = pd.read_csv(files[1])\n",
    "\n",
    "# run datetime convertor\n",
    "hourly_supply = datetime_conv(hourly_supply, date_format='%d/%m/%Y')\n",
    "\n",
    "# rename columns\n",
    "hourly_supply = hourly_supply.rename({'Total Output': \"TOTAL SUPPLY_MW\",\\\n",
    "                                     'NUCLEAR': 'NUCLEAR_MW',\\\n",
    "                                      'GAS': 'GAS_MW',\\\n",
    "                                      'HYDRO': 'HYDRO_MW',\\\n",
    "                                      'WIND': 'WIND_MW',\\\n",
    "                                      'SOLAR': 'SOLAR_MW',\\\n",
    "                                      'BIOFUEL': 'BIOFUEL_MW'\n",
    "                                     },\\\n",
    "                                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache timeseries index\n",
    "index = data.index\n",
    "\n",
    "# merge jonluca and prices\n",
    "data = data.merge(hourly_price,\\\n",
    "                  left_on=[data.index],\\\n",
    "                  right_on=[hourly_price.index],\\\n",
    "                  how='left'\n",
    "                  ).set_index(index) # keep 3 minute datetime index\n",
    "                \n",
    "data = data.drop(['key_0'], axis=1)\n",
    "\n",
    "# merge jonluca/prices and supply\n",
    "data = data.merge(hourly_supply,\\\n",
    "                  left_on=[data.index],\\\n",
    "                  right_on=[hourly_supply.index],\\\n",
    "                  how='left'\n",
    "                  ).set_index(index) # keep 3 minute datetime index\n",
    "\n",
    "data = data.drop(['key_0'], axis=1)\n",
    "\n",
    "data.interpolate(method='time', axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid emission features\n",
    "gas_intensity = 400 # kg/MWh\n",
    "\n",
    "data['GRID EMISSION INTENSITY_kg/MWh'] = (data['GAS_MW'] / data['TOTAL SUPPLY_MW']) * gas_intensity\n",
    "data['GRID EMISSIONS_kgs'] = data['GRID EMISSION INTENSITY_kg/MWh'] * (3/60) # 3 minute intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get current working directory\n",
    "owd = os.getcwd()\n",
    "\n",
    "# clear weather directory\n",
    "weath_dir = '/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/weather_data/*'\n",
    "for file in glob.glob(weath_dir):\n",
    "    os.remove(file)\n",
    "\n",
    "# change to weather data directory\n",
    "os.chdir('/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/weather_data')\n",
    "\n",
    "# clear directory\n",
    "\n",
    "# download weather data from command line\n",
    "os.system('for year in `seq 2020 2020`;do for month in `seq 7 12`;do wget --content-disposition \"https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=51459&Year=${year}&Month=${month}&Day=14&timeframe=1&submit= Download+Data\" ;done;done')\n",
    "os.system('for year in `seq 2021 2021`;do for month in `seq 1 2`;do wget --content-disposition \"https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=51459&Year=${year}&Month=${month}&Day=14&timeframe=1&submit= Download+Data\" ;done;done')\n",
    "\n",
    "# change back to current directory\n",
    "os.chdir(owd)\n",
    "\n",
    "path_weath = '/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/weather_data/*.csv'\n",
    "files = [fname for fname in glob.glob(path_weath)]\n",
    "\n",
    "# read monthly weather data\n",
    "dfs_weath = []\n",
    "for f in files:\n",
    "    month = pd.read_csv(f, header=0)\n",
    "    dfs_weath.append(month)\n",
    "\n",
    "hourly_weath = pd.concat(dfs_weath)\n",
    "hourly_weath = hourly_weath.sort_values(by=['Month', 'Day'])\n",
    "hourly_weath = hourly_weath.rename({'Date/Time (LST)': 'Datetime',\n",
    "                                    'Temp (°C)': 'OUTSIDE TEMP (oC)',\n",
    "                                    'Dew Point Temp (°C)': 'OUTSIDE DEW POINT (oC)',\n",
    "                                    'Rel Hum (%)': 'OUTSIDE HUMIDITY (%)',\n",
    "                                    'Wind Spd (km/h)': 'WIND (km/h)',\n",
    "                                    'Wind Dir (10s deg)': 'WIND DIR (DEGREES)',\n",
    "                                    'Stn Press (kPa)': 'PRESSURE (kPa)'\n",
    "                                   }, axis=1)\n",
    "\n",
    "hourly_weath['Datetime'] = pd.to_datetime(hourly_weath['Datetime'])\n",
    "hourly_weath = hourly_weath.set_index('Datetime')\n",
    "\n",
    "hourly_weath = hourly_weath.drop([\n",
    "    'Longitude (x)',\n",
    "    'Latitude (y)',\n",
    "    'Station Name',\n",
    "    'Climate ID',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'Day',\n",
    "    'Time (LST)',\n",
    "    'Temp Flag',\n",
    "    'Dew Point Temp Flag',\n",
    "    'Rel Hum Flag',\n",
    "    'Precip. Amount (mm)',\n",
    "    'Precip. Amount Flag',\n",
    "    'Wind Dir Flag',\n",
    "    'Wind Spd Flag',\n",
    "    'Visibility (km)',\n",
    "    'Visibility Flag',\n",
    "    'Stn Press Flag',\n",
    "    'Hmdx',\n",
    "    'Hmdx Flag',\n",
    "    'Wind Chill',\n",
    "    'Wind Chill Flag',\n",
    "    'Weather'\n",
    "], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge jonluca/weather\n",
    "data = data.merge(hourly_weath,\\\n",
    "                  left_on=[data.index],\\\n",
    "                  right_on=[hourly_weath.index],\\\n",
    "                  how='left'\n",
    "                  ).set_index(index) # keep 3 minute datetime index\n",
    "\n",
    "data = data.drop(['key_0'], axis=1)\n",
    "data.interpolate(method='time', axis=0, inplace=True)\n",
    "data = data.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle cleaned data\n",
    "path = os.path.join('/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/','cleaned_data')\n",
    "data.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize environment only data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hourly_supply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4ac849435480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/cleaned_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m drop_cols = [hourly_supply.columns,[\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m'GRID EMISSION INTENSITY_kg/MWh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m'GRID EMISSIONS_kgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hourly_supply' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/cleaned_data')\n",
    "\n",
    "drop_cols = [hourly_supply.columns,[\n",
    "'GRID EMISSION INTENSITY_kg/MWh',\n",
    "'GRID EMISSIONS_kgs',\n",
    "'TOTAL POWER (kW)',\n",
    "'TOTAL ENERGY (kWh)'],\n",
    "hourly_price.columns\n",
    "            ]\n",
    "\n",
    "drop_cols = [j for i in drop_cols for j in i]\n",
    "\n",
    "env = data.drop(drop_cols, axis=1)\n",
    "\n",
    "# pickle env\n",
    "# path = os.path.join('/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/','env')\n",
    "# env.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit transform\n",
    "transformer = MinMaxScaler(feature_range=(-1,1))\n",
    "transformer.fit(env)\n",
    "\n",
    "env_norm = transformer.transform(env)\n",
    "env_norm = pd.DataFrame(env_norm, columns=env.columns, index=env.index)\n",
    "\n",
    "# # pickle normalised data\n",
    "# path = os.path.join('/Users/ScottJeen/OneDrive - University of Cambridge/Research/Modelling/emerson_data/','env_norm_data')\n",
    "# env_norm.to_pickle(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
